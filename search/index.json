[{"content":"前言 实验室的机器（下称校内机）是强校内网环境，必须直接连上校园网才能访问和使用（甚至开了校内vpn也连不上）。为了在校外或者断网环境也能优雅的访问机器，我使用了frp内网穿透将内网机器映射到公网服务器上，以此链接内网机器。前排提示，理论上来说这是违规操作，所以请务必做好转发端口的防护以避免内网机器被频繁攻击。\n准备 你需要一个有公网ip的服务器（下称服务器），我用的是阿里云的ECS，2C2G，香港节点，带有一个静态ip。理论上来说动态ip也可以通过DDNS来解析服务并且更加安全，但是我懒得折腾，如果有读者跑通了请务必告诉我qwq。\n公网机器操作 下载并解压frp 1 2 wget \u0026lt;https://github.com/fatedier/frp/releases/download/v0.51.3/frp_0.51.3_linux_amd64.tar.gz\u0026gt; tar -zxvf frp_0.51.3_linux_amd64.tar.gz 配置frps.ini 1 2 3 4 5 6 [common] bind_port = 7000 # frp服务端口 token = your_token # 认证token（自定义，务必复杂一点） dashboard_port = 7500 # 管理面板端口 dashboard_user = admin # 管理员用户名 dashboard_pwd = admin # 管理员密码（自定义，务必复杂一点） 启动frps 1 ./frps -c frps.ini 在ECS控制面板填入方向端口白名单 我是用的是阿里云的ECS，其界面如下。需要手动在控制面板添加7000端口（frp服务端口）和6000（校内网机器指定的公网访问端口）的限制。其中7000端口的授权对象是校内机ip，6000端口建议只开放给自己常用的ip，如果连不上再加，防止潜在的攻击。\n学校服务器（frpc）配置 同样下载并解压frp 1 2 wget \u0026lt;https://github.com/fatedier/frp/releases/download/v0.51.3/frp_0.51.3_linux_amd64.tar.gz\u0026gt; tar -zxvf frp_0.51.3_linux_amd64.tar.gz 配置frpc.ini 1 2 3 4 5 6 7 8 9 10 [common] server_addr = x.x.x.x # 公网服务器IP server_port = 7000 # 对应frps的bind_port token = your_token # 与frps相同的token [ssh] type = tcp local_ip = 127.0.0.1 # 本地IP local_port = 22 # 本地SSH端口 remote_port = 6000 # 公网访问端口 启动frpc 1 ./frpc -c frpc.ini 设置开机自启（可选） 使用systemd创建服务\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 创建frpc.service sudo vim /etc/systemd/system/frpc.service [Unit] Description=frpc daemon After=network.target [Service] Type=simple ExecStart=/path/to/frpc -c /path/to/frpc.ini Restart=always [Install] WantedBy=multi-user.target 启用服务\n1 2 sudo systemctl enable frpc sudo systemctl start frpc 使用 现在，你可以在任何地方使用如下指令链接校内机了\n1 ssh -p 6000 user@public_server_ip 其中user是你校内机的用户名，public_server_ip是公网机器的ip地址。\n","date":"2024-11-20T00:00:00Z","image":"https://whale-dolphin.github.io/p/%E4%BD%BF%E7%94%A8frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E9%9A%8F%E6%97%B6%E9%9A%8F%E5%9C%B0%E8%AE%BF%E9%97%AE%E6%A0%A1%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8/cover_hu3081047382005479862.jpg","permalink":"https://whale-dolphin.github.io/p/%E4%BD%BF%E7%94%A8frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E9%9A%8F%E6%97%B6%E9%9A%8F%E5%9C%B0%E8%AE%BF%E9%97%AE%E6%A0%A1%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8/","title":"使用Frp内网穿透随时随地访问校内网机器"},{"content":"语音评估指标及工具 声音质量指标： MOS (Mean Opinion Score) 虽然这通常是主观评分，但也有自动MOS预测工具，如AutoMOS。\nPESQ (Perceptual Evaluation of Speech Quality)： 用于评估语音质量，特别是在电信系统中。\nSTOI (Short-Time Objective Intelligibility)： 评估语音的可懂度 声音相似度指标： Speaker Similarity Score：通常使用说话人验证模型（如d-vector或x-vector）来计算。 Voice Conversion Score：评估转换后的声音与目标声音的相似度。 发音准确度指标： PER (Phoneme Error Rate)：评估音素级别的准确性。 MCD (Mel Cepstral Distortion)：测量合成语音与参考语音之间的频谱差异。 韵律指标： F0 RMSE (Root Mean Square Error)：评估基频（音高）的准确性 V/UV error (Voiced/Unvoiced error)：评估浊音和清音的判断准确性。 整体性能指标： WER (Word Error Rate)：虽然主要用于ASR，但也可用于评估TTS的可懂度。 CER (Character Error Rate)：类似WER，但在字符级别评估。 特定于声音克隆的指标： EER (Equal Error Rate)：在说话人验证任务中使用，评估克隆声音的欺骗性。 EER是FAR和FRR相等时的错误率。 FAR (False Acceptance Rate) 和 FRR (False Rejection Rate)：在声音克隆任务中。 FAR = (错误接受的克隆声音样本数) / (总克隆声音样本数) FRR(False Rejection Rate)：FRR表示系统错误地拒绝了真实声音的比率。 FRR = (错误拒绝的真实声音样本数) / (总真实声音样本数) 声学特征相似度： MFCC距离：比较原始声音和合成声音的MFCC（Mel频率倒谱系数）。 Spectral Convergence：评估频谱的相似度。 自然度指标： Naturalness MOS：评估合成语音的自然程度。 Prosody MOS：评估韵律的自然度。 测试工具 音色相似度 resemblyzer resemblyzer是一个可以计算音色向量的开源仓库，它使用深度学习模型来提取声音的高级表示，对音频进行decoder从而得到音色向量，通过计算音色向量之间的余弦相似度可以得到两个音频的相似分数。\n使用示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from resemblyzer import VoiceEncoder, preprocess_wav from pathlib import Path # 加载音频文件 wav_fpath = Path(\u0026#34;path/to/audio/file.wav\u0026#34;) wav = preprocess_wav(wav_fpath) # 初始化编码器 encoder = VoiceEncoder() # 提取音频的嵌入向量 embed = encoder.embed_utterance(wav) # 现在可以使用这个嵌入向量进行相似度比较 speechbrain SpeechBrain是另一个强大的开源工具包，用于语音处理任务，包括声音相似度比较。虽然搜索结果中没有直接提到SpeechBrain，但根据我的知识，我可以为您介绍它的一些特点：\nSpeechBrain提供了多种预训练模型，包括说话人识别模型，可以用于声音相似度比较。 它支持提取说话人嵌入向量，这些向量可以用于计算不同音频之间的相似度。 SpeechBrain的模型通常输出说话人嵌入向量，这是一种编码不同人语音相似性的向量表示。[2] 使用示例：\n1 2 3 4 5 6 7 8 9 10 11 12 import torch from speechbrain.pretrained import EncoderClassifier # 加载预训练的说话人识别模型 classifier = EncoderClassifier.from_hparams(source=\u0026#34;speechbrain/spkrec-ecapa-voxceleb\u0026#34;) # 提取两个音频文件的嵌入向量 embedding1 = classifier.encode_batch(torch.tensor([waveform1])) embedding2 = classifier.encode_batch(torch.tensor([waveform2])) # 计算相似度（例如，使用余弦相似度） similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2) 测试下来speechbrain的分数差别比resemblyzer要大，但似乎有些不大准确的地方\nPER (Phoneme Error Rate)\u0026amp;WER(Word Error Rate)测试工具： jiwer jiwer通常用来计算WER和PER\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import jiwer def calculate_per(reference, hypothesis): reference = \u0026#34;sil dh ax cl t r ey n r ae n f ae s cl t sil\u0026#34; hypothesis = \u0026#34;sil dh ax cl t r ey n r ae n s l ow sil\u0026#34; return jiwer(reference, hypothesis) def calculate_chinese_wer(self, reference, hypothesis): \u0026#34;\u0026#34;\u0026#34;计算中文WER\u0026#34;\u0026#34;\u0026#34; reference = clean_text(reference) hypothesis = clean_text(hypothesis) ref_tokens = \u0026#39; \u0026#39;.join(jieba.cut(reference)) hyp_tokens = \u0026#39; \u0026#39;.join(jieba.cut(hypothesis)) ref_tokens = self.transformation(ref_tokens) hyp_tokens = self.transformation(hyp_tokens) return jiwer.wer(ref_tokens, hyp_tokens) leven leven 是一个Python包，可以用来计算PER和WER。它基于Levenshtein距离算法，能够产生与其他标准工具相同的结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 from leven import levenshtein def calculate_error_rate(reference, hypothesis): \u0026#34;\u0026#34;\u0026#34; 计算错误率 (可用于WER或PER) Args: reference: 参考序列(列表) hypothesis: 预测序列(列表) Returns: error_rate: 错误率 num_errors: 编辑距离 \u0026#34;\u0026#34;\u0026#34; # 计算编辑距离 distance = levenshtein(reference, hypothesis) # 错误率 = 编辑距离 / 参考序列长度 error_rate = distance / len(reference) if len(reference) \u0026gt; 0 else 0 return error_rate, distance def calculate_wer(reference_text, hypothesis_text): \u0026#34;\u0026#34;\u0026#34; 计算词错误率 (WER) Args: reference_text: 参考文本 hypothesis_text: 预测文本 Returns: wer: 词错误率 num_errors: 编辑距离 \u0026#34;\u0026#34;\u0026#34; # 将文本分割成单词列表 reference_words = reference_text.strip().split() hypothesis_words = hypothesis_text.strip().split() return calculate_error_rate(reference_words, hypothesis_words) def calculate_per(reference_phones, hypothesis_phones): \u0026#34;\u0026#34;\u0026#34; 计算音素错误率 (PER) Args: reference_phones: 参考音素序列 hypothesis_phones: 预测音素序列 Returns: per: 音素错误率 num_errors: 编辑距离 \u0026#34;\u0026#34;\u0026#34; # 如果输入是字符串，先分割成列表 if isinstance(reference_phones, str): reference_phones = reference_phones.strip().split() if isinstance(hypothesis_phones, str): hypothesis_phones = hypothesis_phones.strip().split() return calculate_error_rate(reference_phones, hypothesis_phones) ","date":"2024-10-26T00:00:00Z","image":"https://whale-dolphin.github.io/p/tts-evaluation-targets/cover_hu9632973327295700706.jpg","permalink":"https://whale-dolphin.github.io/p/tts-evaluation-targets/","title":"TTS Evaluation Targets"},{"content":"VQ(向量量化） VQ是当前语音vocoder的常用技术，其相当于对连续空间表示进行类似聚类处理，让连续表示的信息汇聚到离散值当中，让codebook中的vector都带有确定的信息，方便后面decoder的重建。VQ的第一次提出在VQ-VAE这篇论文中。这篇论文详细的提出了VQ方法并指出VQ能够有效避免后验崩塌问题,确保码本中的每个向量都携带有用信息。此外,这种结构化的离散表示能够帮助decoder在训练过程中更好地重建和补充结构化信息。\nVQ实现的关键在于码本的更新，由于离散的codebook不能直接进行反向传播，所以普遍有两种方法进行码本的更新和整个结构的反向传播：第一种是VQ-VAE论文作者推荐的EMA（指数移动平均）进行更新，该方法也常用于DDPM以及其他模型和架构的更新；第二种是Straight-through估计，就是直接通过一个超参来进行quantized和原向量的加权均值然后计算梯度进行更新。\n代码实现 代码采用随机生成的高斯分布的向量进行VQ操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 import torch import torch.nn.functional as F import torch.optim as optim from torch import nn from torch.utils.data import DataLoader, Dataset import wandb # Initialize wandb wandb.init(project=\u0026#34;vq-single-codebook\u0026#34;, config={ \u0026#34;vector_dim\u0026#34;: 64, \u0026#34;num_vectors\u0026#34;: 10000, \u0026#34;num_embeddings\u0026#34;: 1024, \u0026#34;embedding_dim\u0026#34;: 64, \u0026#34;batch_size\u0026#34;: 32, \u0026#34;num_epochs\u0026#34;: 50, \u0026#34;learning_rate\u0026#34;: 0.0005 }) config = wandb.config # create dataset class VQDataset(Dataset): def __init__(self, vector_dim, num_vectors): super().__init__() data = torch.randn(num_vectors, vector_dim) self.data = data def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] # dataset = vq_Dataset(config[\u0026#39;vector_dim\u0026#39;], config[\u0026#39;num_vectors\u0026#39;]) # vq_DataLoader = DataLoader(dataset, batch_size=32, shuffle=True) # Vector Quantization Model class VQ(nn.Module): def __init__(self, num_embeddings, embedding_dim): super().__init__() self.embedding = nn.Embedding(num_embeddings, embedding_dim) def forward(self, x): x.requires_grad_(True) # print(self.embedding.weight.shape) distances = torch.cdist(x.unsqueeze(1), self.embedding.weight.unsqueeze(0)) indices = torch.argmin(distances, dim=-1) quantized = self.embedding(indices) # Straight-through estimator quantized = quantized + (quantized - x).detach() # Compute loss commitment_loss = F.mse_loss(x, quantized.detach()) codebook_loss = F.mse_loss(quantized, x.detach()) loss = commitment_loss + codebook_loss return quantized, loss, indices # Training function def train_vq(model, dataloader, config): optimizer = optim.Adam(model.parameters(), lr=config.learning_rate) for epoch in range(config.num_epochs): total_loss = 0 for batch in dataloader: optimizer.zero_grad() quantized, loss, _ = model(batch) reconstruction_loss = F.mse_loss(quantized, batch) total_loss = loss + reconstruction_loss total_loss.backward() optimizer.step() wandb.log({ \u0026#34;epoch\u0026#34;: epoch + 1, \u0026#34;total_loss\u0026#34;: total_loss.item(), \u0026#34;commitment_loss\u0026#34;: loss.item(), \u0026#34;reconstruction_loss\u0026#34;: reconstruction_loss.item() }) print(f\u0026#34;Epoch {epoch+1}/{config.num_epochs}, Loss: {total_loss.item():.4f}\u0026#34;) # Create dataset and dataloader dataset = VQDataset(config.vector_dim, config.num_vectors) dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True) # Create and train the VQ model model = VQ(config.num_embeddings, config.embedding_dim) train_vq(model, dataloader, config) # Close wandb run wandb.finish() import matplotlib.pyplot as plt import numpy as np from sklearn.decomposition import PCA def visualize_codebook(model): codebook = model.embedding.weight.detach().cpu().numpy() pca = PCA(n_components=2) codebook_2d = pca.fit_transform(codebook) plt.figure(figsize=(10, 10)) plt.scatter(codebook_2d[:, 0], codebook_2d[:, 1], c=\u0026#39;blue\u0026#39;, marker=\u0026#39;o\u0026#39;) plt.title(\u0026#39;2D Visualization of Codebook\u0026#39;) plt.xlabel(\u0026#39;Principal Component 1\u0026#39;) plt.ylabel(\u0026#39;Principal Component 2\u0026#39;) plt.grid(True) plt.show() visualize_codebook(model) 聚类后的码本的降维可视化\n需要注意的地方 loss的构成 loss由三部分构成，第一部分是commitment_loss，第二部分是codebook_loss，第三部分reconstruction_loss。commitment_loss主要是为了优化输入往码本部分的内容，为了让训练更加稳定；codebook_loss主要是最小化码本损失，最主要的是为了更新码本；reconstruction_loss主要的作用是为了最小化训练过程中的总损失，让重构后的码本能更好的表示输入向量。\nquantized的更新 在quantized的更新过程中需要注意的是梯度的反向传播问题，在\n1 quantized = quantized + (quantized - x).detach() 这行代码中，由于.detach()操作会将向量从计算图中分离，所以要注意的是不能将quantized从计算图中分离从而导致梯度反向传播的时候传不回去。\n","date":"2024-09-26T00:00:00Z","image":"https://whale-dolphin.github.io/p/vector-quatinized/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"https://whale-dolphin.github.io/p/vector-quatinized/","title":"Vector Quatinized"}]