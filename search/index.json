[{"content":"Flow 的核心思路是找到一个从简单先验分布映射到训练数据所在分布的函数，其结构如下图所示：\n相对于 VAE 预设对象的密度分布是连续的所以把先验分布映射到一个简单的隐变量分布上，然后使用 decoder 从隐变量空间学习还原生成后验分布来说，flow 采用一个简单粗暴但在数学上极具美感的做法，它采用一个可逆的方法对先验分布进行学习，暴力的学习先验空间到简单分布的映射，之后用逆方法对其进行生成。个人感觉其相当于把 VAE 的 encoder 和 decoder 揉到一起去了，采用了可逆函数的特点简化了 encoder 和 decoder 对过程。\n生成器 Flow 模型旨在学习一个可逆变换函数 G，该函数将一个简单的先验分布（通常是高斯分布）z 映射到数据分布 x：\n$$ x = G(z) $$由于 G 是可逆的，我们可以得到逆变换：\n$$ z = G⁻¹(x) $$这种可逆性是 Flow 模型的关键，它允许我们直接计算数据 x 的概率密度：\n$$ p(x)=\\pi (z)\\left|\\det J_{G^{-1}} \\right| $$其中 $p(z)$ 是先验分布的概率密度，$J_{G^{-1}}$ 是逆变换 $G⁻¹$ 的雅可比矩阵。\nflow 优化真实值在后验分布中的最大似然，公式如下：\n$$ G^*=arg max \\displaystyle\\sum^m_{i=1}logP_G(x_i) $$Change of variable Theorem $$ p(x') = \\pi(z') \\left| \\frac{dz}{dx} \\right| $$\n$$ p(x') \\left| \\det \\begin{bmatrix} \\Delta x_{11} \u0026 \\Delta x_{21} \\\\ \\Delta x_{12} \u0026 \\Delta x_{22}\\end{bmatrix} \\right| = \\pi(z') \\Delta z_{1} \\Delta z_{2} $$$$ p(x')=\\pi (z')\\left|\\det J_{f^{-1}} \\right| $$Jabobian 矩阵 Jacobian 矩阵是一个函数的所有一阶偏导数的矩阵。对于从$R^n$映射到$R^m$的函数，Jacobian 矩阵的维度是$m \\times n$。\n对于函数 $F: R^n \\rightarrow R^m$，其中 $F(x) = [f_1(x), f_2(x), \u0026hellip;, f_m(x)]^T$， Jacobian 矩阵 J 表示为：\n$$ J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026 \\frac{\\partial f_1}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} \u0026 \\frac{\\partial f_2}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} \u0026 \\frac{\\partial f_m}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$几何意义\n线性逼近： 雅可比矩阵代表了函数 f 在给定点 x 的最佳线性逼近。也就是说，当 x 发生一个微小变化 Δx 时，f(x) 的变化 Δf 可以近似表示为： $$ Δf ≈ J Δx $$这个公式类似于单变量微积分中的导数概念，雅可比矩阵相当于多变量函数的“导数”。\n局部变换： 雅可比矩阵描述了函数 f 在 x 附近的局部变换。它反映了输入空间的微小变化如何在输出空间中被拉伸、压缩或旋转。 体积变化： 雅可比矩阵的行列式 $|det(J)|$ (当 $m = n$ 时，雅可比矩阵为方阵) 表示函数 $f$ 在 $x$ 附近对体积的改变程度。如果 $|det(J)| \u0026gt; 1$，则表示体积被放大；如果 $0 \u0026lt; |det(J)| \u0026lt; 1$，则表示体积被缩小；如果 $|det(J)| = 0$，则表示变换将输入空间映射到一个低维空间。 Flow-base 最终的优化目标是$P_G(x_i)$，由上式可知：\n$$ p(x)=\\pi(\\mathbb{G}^{-1}(x))\\left|\\det J_{\\mathbb{G}^{-1}} \\right| $$两边取对数可得：\n$$ \\log p_{\\mathbb{G}}(x) = \\log \\pi(\\mathbb{G}^{-1}(x)) + \\log |det(J_{\\mathbb{G}^{-1}})| $$Coupling Layer 为了解决生成器可逆的问题，flow 采用了如下结构的生成器\n其中，分别讲输出和输出分成两个部分，对于对于原函数计算，我们将第一个部分直接复制，然后通过两个神经网络得到$\\beta$和$\\gamma$，然后通过$x=z\\cdot \\beta + \\gamma$计算$x$值。反函数的计算直接复制第一部分，第二个部分相减即可\n解决了生成器的反函数问题，优化最大似然的目标就是找到生成器的 Jabobian 矩阵，生成器的 Jabobian 矩阵计算如下：\n对于第一个部分，易知其为单位矩阵，$z$的第二部分和第一部分无关，所以为零矩阵，此时整个函数的 Jabobian 矩阵只与右下角这一部分有关，右下角部分逐个求偏导就是$\\beta$的值，故整个函数的 Jabobian 矩阵可写作如下形式：\n$$ ⁍ $$Coupling Layer-Stacking 我们通常将多个 flow 模型堆叠起来进行使用，这会产生一个问题：每次只有二部分参与变换，所以我们采用 Coupling Layer-Stacking，也就是每次生成器会切换不同的部分进行变换。\n","date":"2024-12-16T00:00:00Z","image":"https://whale-dolphin.github.io/p/flow-base/cover_hu7371877127337383529.jpg","permalink":"https://whale-dolphin.github.io/p/flow-base/","title":"Flow-base"},{"content":"前言 实验室的机器（下称校内机）是强校内网环境，必须直接连上校园网才能访问和使用（甚至开了校内vpn也连不上）。为了在校外或者断网环境也能优雅的访问机器，我使用了frp内网穿透将内网机器映射到公网服务器上，以此链接内网机器。前排提示，理论上来说这是违规操作，所以请务必做好转发端口的防护以避免内网机器被频繁攻击。\n准备 你需要一个有公网ip的服务器（下称服务器），我用的是阿里云的ECS，2C2G，香港节点，带有一个静态ip。理论上来说动态ip也可以通过DDNS来解析服务并且更加安全，但是我懒得折腾，如果有读者跑通了请务必告诉我qwq。\n公网机器操作 下载并解压frp 1 2 wget \u0026lt;https://github.com/fatedier/frp/releases/download/v0.51.3/frp_0.51.3_linux_amd64.tar.gz\u0026gt; tar -zxvf frp_0.51.3_linux_amd64.tar.gz 配置frps.ini 1 2 3 4 5 6 [common] bind_port = 7000 # frp服务端口 token = your_token # 认证token（自定义，务必复杂一点） dashboard_port = 7500 # 管理面板端口 dashboard_user = admin # 管理员用户名 dashboard_pwd = admin # 管理员密码（自定义，务必复杂一点） 启动frps 1 ./frps -c frps.ini 在ECS控制面板填入方向端口白名单 我是用的是阿里云的ECS，其界面如下。需要手动在控制面板添加7000端口（frp服务端口）和6000（校内网机器指定的公网访问端口）的限制。其中7000端口的授权对象是校内机ip，6000端口建议只开放给自己常用的ip，如果连不上再加，防止潜在的攻击。\nCIDR规则 由于本人经常在校内校外跑，一个个添加ip极其麻烦。折腾了一会才知道ip有通配符这个东西，遂添加关于CIDR规则的描述。CIDR就是类似于x.x.x.x/x的形式，前四位是模糊ip地址，只用填确定的地址的数字即可，不确定的用任意数字表示（一般是0），/后面表示前面多少位是固定的。\n以我学校为例，我们学校教育网ip的ip段为115.156.0.0-115.157.127.255，写成CIDR格式即为115.156.0.0/15。\n学校服务器（frpc）配置 同样下载并解压frp 1 2 wget \u0026lt;https://github.com/fatedier/frp/releases/download/v0.51.3/frp_0.51.3_linux_amd64.tar.gz\u0026gt; tar -zxvf frp_0.51.3_linux_amd64.tar.gz 配置frpc.ini 1 2 3 4 5 6 7 8 9 10 [common] server_addr = x.x.x.x # 公网服务器IP server_port = 7000 # 对应frps的bind_port token = your_token # 与frps相同的token [ssh] type = tcp local_ip = 127.0.0.1 # 本地IP local_port = 22 # 本地SSH端口 remote_port = 6000 # 公网访问端口 启动frpc 1 ./frpc -c frpc.ini 设置开机自启（可选） 使用systemd创建服务\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 创建frpc.service sudo vim /etc/systemd/system/frpc.service [Unit] Description=frpc daemon After=network.target [Service] Type=simple ExecStart=/path/to/frpc -c /path/to/frpc.ini Restart=always [Install] WantedBy=multi-user.target 启用服务\n1 2 sudo systemctl enable frpc sudo systemctl start frpc 使用 现在，你可以在任何地方使用如下指令链接校内机了\n1 ssh -p 6000 user@public_server_ip 其中user是你校内机的用户名，public_server_ip是公网机器的ip地址。\n","date":"2024-11-20T00:00:00Z","image":"https://whale-dolphin.github.io/p/%E4%BD%BF%E7%94%A8frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E9%9A%8F%E6%97%B6%E9%9A%8F%E5%9C%B0%E8%AE%BF%E9%97%AE%E6%A0%A1%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8/cover_hu3081047382005479862.jpg","permalink":"https://whale-dolphin.github.io/p/%E4%BD%BF%E7%94%A8frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E9%9A%8F%E6%97%B6%E9%9A%8F%E5%9C%B0%E8%AE%BF%E9%97%AE%E6%A0%A1%E5%86%85%E7%BD%91%E6%9C%BA%E5%99%A8/","title":"使用Frp内网穿透随时随地访问校内网机器"},{"content":"语音评估指标及工具 声音质量指标： MOS (Mean Opinion Score) 虽然这通常是主观评分，但也有自动MOS预测工具，如AutoMOS。\nPESQ (Perceptual Evaluation of Speech Quality)： 用于评估语音质量，特别是在电信系统中。\nSTOI (Short-Time Objective Intelligibility)： 评估语音的可懂度 声音相似度指标： Speaker Similarity Score：通常使用说话人验证模型（如d-vector或x-vector）来计算。 Voice Conversion Score：评估转换后的声音与目标声音的相似度。 发音准确度指标： PER (Phoneme Error Rate)：评估音素级别的准确性。 MCD (Mel Cepstral Distortion)：测量合成语音与参考语音之间的频谱差异。 韵律指标： F0 RMSE (Root Mean Square Error)：评估基频（音高）的准确性 V/UV error (Voiced/Unvoiced error)：评估浊音和清音的判断准确性。 整体性能指标： WER (Word Error Rate)：虽然主要用于ASR，但也可用于评估TTS的可懂度。 CER (Character Error Rate)：类似WER，但在字符级别评估。 特定于声音克隆的指标： EER (Equal Error Rate)：在说话人验证任务中使用，评估克隆声音的欺骗性。 EER是FAR和FRR相等时的错误率。 FAR (False Acceptance Rate) 和 FRR (False Rejection Rate)：在声音克隆任务中。 FAR = (错误接受的克隆声音样本数) / (总克隆声音样本数) FRR(False Rejection Rate)：FRR表示系统错误地拒绝了真实声音的比率。 FRR = (错误拒绝的真实声音样本数) / (总真实声音样本数) 声学特征相似度： MFCC距离：比较原始声音和合成声音的MFCC（Mel频率倒谱系数）。 Spectral Convergence：评估频谱的相似度。 自然度指标： Naturalness MOS：评估合成语音的自然程度。 Prosody MOS：评估韵律的自然度。 测试工具 音色相似度 resemblyzer resemblyzer是一个可以计算音色向量的开源仓库，它使用深度学习模型来提取声音的高级表示，对音频进行decoder从而得到音色向量，通过计算音色向量之间的余弦相似度可以得到两个音频的相似分数。\n使用示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from resemblyzer import VoiceEncoder, preprocess_wav from pathlib import Path # 加载音频文件 wav_fpath = Path(\u0026#34;path/to/audio/file.wav\u0026#34;) wav = preprocess_wav(wav_fpath) # 初始化编码器 encoder = VoiceEncoder() # 提取音频的嵌入向量 embed = encoder.embed_utterance(wav) # 现在可以使用这个嵌入向量进行相似度比较 speechbrain SpeechBrain是另一个强大的开源工具包，用于语音处理任务，包括声音相似度比较。虽然搜索结果中没有直接提到SpeechBrain，但根据我的知识，我可以为您介绍它的一些特点：\nSpeechBrain提供了多种预训练模型，包括说话人识别模型，可以用于声音相似度比较。 它支持提取说话人嵌入向量，这些向量可以用于计算不同音频之间的相似度。 SpeechBrain的模型通常输出说话人嵌入向量，这是一种编码不同人语音相似性的向量表示。[2] 使用示例：\n1 2 3 4 5 6 7 8 9 10 11 12 import torch from speechbrain.pretrained import EncoderClassifier # 加载预训练的说话人识别模型 classifier = EncoderClassifier.from_hparams(source=\u0026#34;speechbrain/spkrec-ecapa-voxceleb\u0026#34;) # 提取两个音频文件的嵌入向量 embedding1 = classifier.encode_batch(torch.tensor([waveform1])) embedding2 = classifier.encode_batch(torch.tensor([waveform2])) # 计算相似度（例如，使用余弦相似度） similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2) 测试下来speechbrain的分数差别比resemblyzer要大，但似乎有些不大准确的地方\nPER (Phoneme Error Rate)\u0026amp;WER(Word Error Rate)测试工具： jiwer jiwer通常用来计算WER和PER\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import jiwer def calculate_per(reference, hypothesis): reference = \u0026#34;sil dh ax cl t r ey n r ae n f ae s cl t sil\u0026#34; hypothesis = \u0026#34;sil dh ax cl t r ey n r ae n s l ow sil\u0026#34; return jiwer(reference, hypothesis) def calculate_chinese_wer(self, reference, hypothesis): \u0026#34;\u0026#34;\u0026#34;计算中文WER\u0026#34;\u0026#34;\u0026#34; reference = clean_text(reference) hypothesis = clean_text(hypothesis) ref_tokens = \u0026#39; \u0026#39;.join(jieba.cut(reference)) hyp_tokens = \u0026#39; \u0026#39;.join(jieba.cut(hypothesis)) ref_tokens = self.transformation(ref_tokens) hyp_tokens = self.transformation(hyp_tokens) return jiwer.wer(ref_tokens, hyp_tokens) leven leven 是一个Python包，可以用来计算PER和WER。它基于Levenshtein距离算法，能够产生与其他标准工具相同的结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 from leven import levenshtein def calculate_error_rate(reference, hypothesis): \u0026#34;\u0026#34;\u0026#34; 计算错误率 (可用于WER或PER) Args: reference: 参考序列(列表) hypothesis: 预测序列(列表) Returns: error_rate: 错误率 num_errors: 编辑距离 \u0026#34;\u0026#34;\u0026#34; # 计算编辑距离 distance = levenshtein(reference, hypothesis) # 错误率 = 编辑距离 / 参考序列长度 error_rate = distance / len(reference) if len(reference) \u0026gt; 0 else 0 return error_rate, distance def calculate_wer(reference_text, hypothesis_text): \u0026#34;\u0026#34;\u0026#34; 计算词错误率 (WER) Args: reference_text: 参考文本 hypothesis_text: 预测文本 Returns: wer: 词错误率 num_errors: 编辑距离 \u0026#34;\u0026#34;\u0026#34; # 将文本分割成单词列表 reference_words = reference_text.strip().split() hypothesis_words = hypothesis_text.strip().split() return calculate_error_rate(reference_words, hypothesis_words) def calculate_per(reference_phones, hypothesis_phones): \u0026#34;\u0026#34;\u0026#34; 计算音素错误率 (PER) Args: reference_phones: 参考音素序列 hypothesis_phones: 预测音素序列 Returns: per: 音素错误率 num_errors: 编辑距离 \u0026#34;\u0026#34;\u0026#34; # 如果输入是字符串，先分割成列表 if isinstance(reference_phones, str): reference_phones = reference_phones.strip().split() if isinstance(hypothesis_phones, str): hypothesis_phones = hypothesis_phones.strip().split() return calculate_error_rate(reference_phones, hypothesis_phones) ","date":"2024-10-26T00:00:00Z","image":"https://whale-dolphin.github.io/p/tts-evaluation-targets/cover_hu9632973327295700706.jpg","permalink":"https://whale-dolphin.github.io/p/tts-evaluation-targets/","title":"TTS Evaluation Targets"},{"content":"VQ(向量量化） VQ是当前语音vocoder的常用技术，其相当于对连续空间表示进行类似聚类处理，让连续表示的信息汇聚到离散值当中，让codebook中的vector都带有确定的信息，方便后面decoder的重建。VQ的第一次提出在VQ-VAE这篇论文中。这篇论文详细的提出了VQ方法并指出VQ能够有效避免后验崩塌问题,确保码本中的每个向量都携带有用信息。此外,这种结构化的离散表示能够帮助decoder在训练过程中更好地重建和补充结构化信息。\nVQ实现的关键在于码本的更新，由于离散的codebook不能直接进行反向传播，所以普遍有两种方法进行码本的更新和整个结构的反向传播：第一种是VQ-VAE论文作者推荐的EMA（指数移动平均）进行更新，该方法也常用于DDPM以及其他模型和架构的更新；第二种是Straight-through估计，就是直接通过一个超参来进行quantized和原向量的加权均值然后计算梯度进行更新。\n代码实现 代码采用随机生成的高斯分布的向量进行VQ操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 import torch import torch.nn.functional as F import torch.optim as optim from torch import nn from torch.utils.data import DataLoader, Dataset import wandb # Initialize wandb wandb.init(project=\u0026#34;vq-single-codebook\u0026#34;, config={ \u0026#34;vector_dim\u0026#34;: 64, \u0026#34;num_vectors\u0026#34;: 10000, \u0026#34;num_embeddings\u0026#34;: 1024, \u0026#34;embedding_dim\u0026#34;: 64, \u0026#34;batch_size\u0026#34;: 32, \u0026#34;num_epochs\u0026#34;: 50, \u0026#34;learning_rate\u0026#34;: 0.0005 }) config = wandb.config # create dataset class VQDataset(Dataset): def __init__(self, vector_dim, num_vectors): super().__init__() data = torch.randn(num_vectors, vector_dim) self.data = data def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] # dataset = vq_Dataset(config[\u0026#39;vector_dim\u0026#39;], config[\u0026#39;num_vectors\u0026#39;]) # vq_DataLoader = DataLoader(dataset, batch_size=32, shuffle=True) # Vector Quantization Model class VQ(nn.Module): def __init__(self, num_embeddings, embedding_dim): super().__init__() self.embedding = nn.Embedding(num_embeddings, embedding_dim) def forward(self, x): x.requires_grad_(True) # print(self.embedding.weight.shape) distances = torch.cdist(x.unsqueeze(1), self.embedding.weight.unsqueeze(0)) indices = torch.argmin(distances, dim=-1) quantized = self.embedding(indices) # Straight-through estimator quantized = quantized + (quantized - x).detach() # Compute loss commitment_loss = F.mse_loss(x, quantized.detach()) codebook_loss = F.mse_loss(quantized, x.detach()) loss = commitment_loss + codebook_loss return quantized, loss, indices # Training function def train_vq(model, dataloader, config): optimizer = optim.Adam(model.parameters(), lr=config.learning_rate) for epoch in range(config.num_epochs): total_loss = 0 for batch in dataloader: optimizer.zero_grad() quantized, loss, _ = model(batch) reconstruction_loss = F.mse_loss(quantized, batch) total_loss = loss + reconstruction_loss total_loss.backward() optimizer.step() wandb.log({ \u0026#34;epoch\u0026#34;: epoch + 1, \u0026#34;total_loss\u0026#34;: total_loss.item(), \u0026#34;commitment_loss\u0026#34;: loss.item(), \u0026#34;reconstruction_loss\u0026#34;: reconstruction_loss.item() }) print(f\u0026#34;Epoch {epoch+1}/{config.num_epochs}, Loss: {total_loss.item():.4f}\u0026#34;) # Create dataset and dataloader dataset = VQDataset(config.vector_dim, config.num_vectors) dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True) # Create and train the VQ model model = VQ(config.num_embeddings, config.embedding_dim) train_vq(model, dataloader, config) # Close wandb run wandb.finish() import matplotlib.pyplot as plt import numpy as np from sklearn.decomposition import PCA def visualize_codebook(model): codebook = model.embedding.weight.detach().cpu().numpy() pca = PCA(n_components=2) codebook_2d = pca.fit_transform(codebook) plt.figure(figsize=(10, 10)) plt.scatter(codebook_2d[:, 0], codebook_2d[:, 1], c=\u0026#39;blue\u0026#39;, marker=\u0026#39;o\u0026#39;) plt.title(\u0026#39;2D Visualization of Codebook\u0026#39;) plt.xlabel(\u0026#39;Principal Component 1\u0026#39;) plt.ylabel(\u0026#39;Principal Component 2\u0026#39;) plt.grid(True) plt.show() visualize_codebook(model) 聚类后的码本的降维可视化\n需要注意的地方 loss的构成 loss由三部分构成，第一部分是commitment_loss，第二部分是codebook_loss，第三部分reconstruction_loss。commitment_loss主要是为了优化输入往码本部分的内容，为了让训练更加稳定；codebook_loss主要是最小化码本损失，最主要的是为了更新码本；reconstruction_loss主要的作用是为了最小化训练过程中的总损失，让重构后的码本能更好的表示输入向量。\nquantized的更新 在quantized的更新过程中需要注意的是梯度的反向传播问题，在\n1 quantized = quantized + (quantized - x).detach() 这行代码中，由于.detach()操作会将向量从计算图中分离，所以要注意的是不能将quantized从计算图中分离从而导致梯度反向传播的时候传不回去。\n","date":"2024-09-26T00:00:00Z","image":"https://whale-dolphin.github.io/p/vector-quatinized/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"https://whale-dolphin.github.io/p/vector-quatinized/","title":"Vector Quatinized"},{"content":"第一章 计算机系统概论 计算机发展历程 计算机的四代发展 第一代(1945-1955) 特征:电子管、插板式编程 编程方式:机器语言 主要用途:数值计算 第二代(1955-1965) 特征:晶体管、批处理系统 编程方式:汇编语言、FORTRAN 代表机型:IBM 7094 第三代(1965-1980) 特征:集成电路、多道程序、分时系统 编程方式:高级语言(BASIC、C) 技术创新:多道程序设计、分时处理 第四代(1980-至今) 特征:超大规模集成电路 发展方向: 个人计算机 并行计算 分布式系统 云计算 计算机系统的层次结构 计算机系统的五个层次 高级语言层(M4) 汇编语言层(M3) 操作系统层(M2) 机器语言层(M1) 微程序层(M0) 性能指标 主要性能指标 CPI(Cycles Per Instruction) 基本公式 / Basic Formula\n中文：CPI = 总时钟周期数 / 总指令数 English: CPI = Total Clock Cycles / Total Instructions 表示为数学公式 / Mathematical expression: $CPI=\\frac{Clock Cycles}{Instruction Count}$\n加权平均公式 / Weighted Average Formula\n当有多种类型的指令时 / When there are multiple instruction types:\n$CPI=\\sum_{i=1}^{n}(CPI_i×F_i)$\nMIPS(Million Instructions Per Second) 基本公式 / Basic Formula $MIPS=\\frac{指令数执行}{时间×10^6}=\\frac{时钟频率}{CPI×10^6}$\n英文表示 / In English: $MIPS=\\frac{Instruction Count}{Execution Time×10^6}=\\frac{Clock Frequency}{CPI×10^6}$\n扩展公式 / Extended Formula 当已知时钟频率（Hz）和CPI时：/ When clock frequency (Hz) and CPI are known:\n$MIPS=\\frac{Clock Frequency (Hz)}{CPI×10^6}$\n重要例题分析 例题1: CPI计算 1 2 3 4 5 6 7 8 9 10 11 问题:某程序的目标代码由4类指令组成: - 算术逻辑运算(60%, CPI=1) - 内存读写(18%, CPI=2) - 转移(12%, CPI=4) - 其它(10%, CPI=8) 求该程序的平均CPI。 解答: CPI = 1×0.6 + 2×0.18 + 4×0.12 + 8×0.1 = 0.6 + 0.36 + 0.48 + 0.8 = 2.24 例题2: CPU性能计算 $$ N=t\\times f $$ N：时钟周期总数 t：运行时间 f：频率 1 2 3 4 5 6 7 8 9 10 11 12 问题:程序P在机器A上运行需10s,机器A的时钟频率为400MHz。 如果在机器B上运行只需6s,且B的时钟周期总数是A的1.2倍, 求B的时钟频率是A的多少倍? 解答: 1. 计算A的时钟周期总数: 时钟周期总数A = 10s × 400MHz = 4000M个 2. 计算B的时钟频率: 时钟频率B = (1.2 × 4000M) ÷ 6s = 800MHz 3. 比值 = 800MHz ÷ 400MHz = 2倍 例题3: MIPS计算 1 2 3 4 5 6 7 8 9 10 11 问题： 某计算机系统运行一个程序，CPU时钟频率2GHz，执行了5×10⁸条指令，平均CPI为2.5， 程序总执行时间0.625秒。 求：(1)该程序的MIPS值； (2)若CPU频率提高到2.5GHz，新的MIPS值是多少？ 解： (1) MIPS = 指令数/(执行时间×10⁶) = 5×10⁸/(0.625×10⁶) = 800 (2) 新执行时间 = 原执行时间×(原频率/新频率) = 0.625×(2/2.5) = 0.5s 新MIPS = 5×10⁸/(0.5×10⁶) = 1000 冯·诺依曼计算机的基本特点 五大部件 运算器(ALU) 控制器(CU) 存储器(Memory) 输入设备(Input) 输出设备(Output) 重要特征 计算机由五大部件组成 指令和数据以二进制表示 指令和数据存放在同一存储器中 指令由操作码和地址码组成 存储程序 按地址访问 第二章 计算机数据表示 数据表示基础 为什么使用二进制 易于实现：只有0和1两种状态 抗干扰能力强 便于逻辑运算 硬件实现简单 数据表示考虑因素 数据类型（数值/非数值） 表示范围和精度 存储和处理代价 软件可移植性 数值数据表示 机器数的编码方式 原码 反码 补码 移码 原码 最高位为符号位（0正1负）\n其余位是数值的绝对值\n公式：\n$$ [X]_{\\text{原}} = \\begin{cases} X, \u0026 0 \\leq X \u003c 2^n \\\\ 2^n - |X|, \u0026 -2^n \u003c X \\leq 0 \\end{cases} $$ 反码 正数的反码与原码相同\n负数的反码是对原码除符号位外各位取反\n公式：\n$$ [X]_{\\text{反}} = \\begin{cases} X, \u0026 0 \\leq X \u003c 2^n \\\\ 2^{n+1}-1 + X, \u0026 -2^n \u003c X \\leq 0 \\end{cases} $$ 补码 正数的补码与原码相同\n负数的补码是在反码的基础上末位加1\n公式：\n$$ [X]_{\\text{补}} = \\begin{cases} X, \u0026 0 \\leq X \u003c 2^n \\\\ 2^{n+1} + X, \u0026 -2^n \u003c X \\leq 0 \\end{cases} $$ 移码 常用于表示浮点数的阶码 公式：$[x]_移 = x + 2^{n-1}$ 与补码的关系：符号位取反其余位不变 例题 示例1：将十进制数 +52 转换为8位二进制机器码 原码 正数，符号位为0 52的二进制为110100 8位原码表示：0110,1000 反码 正数的反码等于原码 所以反码也是：0110,1000 补码 正数的补码等于原码 所以补码也是：0110,1000 示例2：将十进制数 -52 转换为8位二进制机器码 原码\n负数，符号位为1 |52|的二进制为110100 8位原码表示：1110,1000 反码\n负数，符号位为1\n数值位按位取反\n计算过程\n1 2 原码：1110,1000 数值位取反：1001,0111 补码\n负数，在反码基础上末位加1\n计算过程\n1 2 反码：1001,0111 末位加1：1001,1000 转换口诀 正数：原码 = 反码 = 补码 负数： 原码：符号位为1，其余为绝对值的二进制 反码：符号位为1，数值位按位取反 补码：反码末位加1 整型和浮点型表示 整型 整型在计算机中使用补码表示\n1 2 3 4 5 类型 位数 范围 byte 8位 -128 到 127 short 16位 -32,768 到 32,767 int 32位 -2^31 到 2^31-1 long 64位 -2^63 到 2^63-1 浮点数 $$ V=(-1)^s \\times M \\times 2^E $$其中：\nS：符号位（0正1负） M：1.尾数（规范化的尾数） E：指数值（减去偏置值的指数） 例子：存储float类型的12.375 转换为二进制：\n1 2 3 12 = 1100(二进制) 0.375 = 0.011(二进制) 12.375 = 1100.011(二进制) 规范化：\n1 1100.011 = 1.100011 × 2^3 存储格式：\n1 2 3 4 5 符号位(S)：0（正数） 指数位(E)：3 + 127(偏置) = 130 = 10000010 尾数位(M)：100011（不存储小数点前的1） 最终存储：0 10000010 10001100000000000000000 字节序 大端序(Big Endian)：高位字节存储在低地址 小端序(Little Endian)：低位字节存储在低地址 1 2 3 例如：整数 0x12345678 大端序：12 34 56 78 小端序：78 56 34 12 数据校验 码距 码距是指两个等长编码之间不同位的个数 最小码距是指一个编码系统中任意两个合法编码之间的最小距离 1 2 3 4 例如： 编码A：1 0 1 1 0 编码B：1 1 1 0 0 码距 = 2（第2位和第4位不同） 奇偶校验 偶校验 / Even Parity 原理：确保数据位中\u0026quot;1\u0026quot;的总数（包括校验位）为偶数 校验位设置：如果数据位中\u0026quot;1\u0026quot;的个数为奇数，则校验位设为1；如果为偶数，则设为0 奇校验 / Odd Parity 原理：确保数据位中\u0026quot;1\u0026quot;的总数（包括校验位）为奇数 校验位设置：如果数据位中\u0026quot;1\u0026quot;的个数为偶数，则校验位设为1；如果为奇数，则设为0 海明码 校验码位置 校验位放在2的幂次位置上（1,2,4,8,\u0026hellip;） 数据位放在其他位置上 1 2 3 位置： 1 2 3 4 5 6 7 8 9 10 11 类型： p1 p2 d1 p3 d2 d3 d4 p4 d5 d6 d7 (p=校验位, d=数据位) 校验码数量 对于k位数据，需要r位校验位，满足：\n$$ 2^r ≥ k + r + 1 $$校验码负责校验的位置 p1(位置1): 检查二进制第1位为1的位置\n1 001(1), 011(3), 101(5), 111(7) p2(位置2): 检查二进制第2位为1的位置\n1 010(2), 011(3), 110(6), 111(7) p3(位置4): 检查二进制第3位为1的位置\n1 100(4), 101(5), 110(6), 111(7) 校验码的值 校验码就是在负责校验位置上的偶校验\n对单个错误进行纠错 计算综合征\n第一位综合征(s1) 检查所有奇数位置的位 s1 = p1 ⊕ d1 ⊕ d2 ⊕ d4 第二位综合征(s2) 检查位置2,3,6,7的位 s2 = p2 ⊕ d1 ⊕ d3 ⊕ d4 第三位综合征(s3) 检查位置4,5,6,7的位 s3 = p3 ⊕ d2 ⊕ d3 ⊕ d4 以此类推\n综合征含义\n1 2 3 4 5 6 7 8 9 综合征值(s3s2s1) 表示含义 000 无错误 001 位置1出错 010 位置2出错 011 位置3出错 100 位置4出错 101 位置5出错 110 位置6出错 111 位置7出错 以此类推\n例题 海明码实例：4位数据的(7,4)海明码 原始数据\n假设要传输的数据是：1011\n生成海明码\n确定位置\n1 2 3 4 5 位置： 1 2 3 4 5 6 7 用途： p1 p2 d1 p3 d2 d3 d4 数据： p1 p2 1 p3 0 1 1 ^ ^ ^ ^ ^ ^ ^ 二进制： 001 010 011 100 101 110 111 计算校验位\n计算p1（检查1,3,5,7位） p1 ⊕ d1 ⊕ d2 ⊕ d4 = 0 p1 ⊕ 1 ⊕ 0 ⊕ 1 = 0 p1 = 1 ⊕ 0 ⊕ 1 = 0 计算p2（检查2,3,6,7位） p2 ⊕ d1 ⊕ d3 ⊕ d4 = 0 p2 ⊕ 1 ⊕ 1 ⊕ 1 = 0 p2 = 1 ⊕ 1 ⊕ 1 = 1 计算p3（检查4,5,6,7位） p3 ⊕ d2 ⊕ d3 ⊕ d4 = 0 p3 ⊕ 0 ⊕ 1 ⊕ 1 = 0 p3 = 0 ⊕ 1 ⊕ 1 = 0 最终海明码\n1 2 位置： 1 2 3 4 5 6 7 数据： 0 1 1 0 0 1 1 完整的海明码是：0110011\n错误检测示例\n3.1 假设第5位发生错误\n接收到的错误数据：0110111（第5位从0变成了1）\n3.2 计算综合征\n计算s1（检查1,3,5,7位） 0 ⊕ 1 ⊕ 1 ⊕ 1 = 1 计算s2（检查2,3,6,7位） 1 ⊕ 1 ⊕ 1 ⊕ 1 = 0 计算s3（检查4,5,6,7位） 0 ⊕ 1 ⊕ 1 ⊕ 1 = 1 3.3 错误定位\n得到的综合征：101 对照综合征表：101 表示第5位出错 将第5位取反即可纠正错误 3.4 纠错结果\n1 2 3 错误数据： 0 1 1 0 1 1 1 纠正后： 0 1 1 0 0 1 1 位置： 1 2 3 4 5 6 7 验证正确性\n4.1 重新检验\n计算三个校验方程：\ns1 = 0 ⊕ 1 ⊕ 0 ⊕ 1 = 0 s2 = 1 ⊕ 1 ⊕ 1 ⊕ 1 = 0 s3 = 0 ⊕ 0 ⊕ 1 ⊕ 1 = 0 所有校验方程结果都为0，说明纠错成功。\n4.2 提取原始数据\n取出数据位（第3,5,6,7位）：1011 与原始数据相同，验证成功 CRC循环冗余校验 校验流程 确定CRC位数（r）\n应满足：$2^r\u0026gt;数据长度+r$\n选择合适的CRC标准\nCRC-16 生成多项式：$x^{16}+x^{15}+x^{2}+1$ 广泛应用于USB、Modbus等协议 CRC-32 生成多项式：$x^{32}+x^{26}+x^{23}+x^{22}+x^{16}+x^{12}+x^{11}+x^{10}+x^{8}+x^7+x^5+x^4+x^2+x+1$ 用于以太网、ZIP文件等 CRC-8 生成多项式：$x^8+x^7+x^6+x^4+x^2+1$ 用于简单的数据校验 CRC-4 生成多项式：$x^3+x+1$ 数据预处理\n补零操作（补生成多项式度数-1个零），并接到原数据后面\n求余数\n用补完0的数除以生成多项式的二进制表示来求得余数，并将余数替换后面补的0\n判断是否正确\n通过传入的数除以CRC方法对应的多项式来判断是否正确，余0表示正确\nCRC循环校验的优势和劣势 优势\n能够检测所有单比特错误 能够检测所有双比特错误 能够检测大多数突发错误 能检测所有奇数个数错误 劣势\n无法纠错 存在理论上的盲区 例题 CRC计算 题目：数据1100，生成多项式G(x)=1011，求CRC码。\n步骤：\n数据左移3位：1100000\n模2除法：\n1 2 3 4 5 6 7 8 9 10 1100000÷1011 1011 ----- 1110 1011 ----- 1010 1011 ----- 0010 (余数) CRC码：1100010\n第三章 运算方法与运算器 定点加减法运算 补码加法运算 (Addition in Two\u0026rsquo;s Complement) 补码的加法非常直接，直接按位相加，溢出位舍弃。 Addition in two\u0026rsquo;s complement is straightforward - add bits directly and discard overflow.\n例如 (Example)：\n1 2 3 4 5 的补码 (TC of 5): 0101 -3 的补码 (TC of -3): 1101 ------------------------- 相加结果 (Sum): 0010 (= 2) 补码减法运算 (Subtraction in Two\u0026rsquo;s Complement) 减法可以转换为加上负数的补码：A - B = A + (-B) Subtraction can be converted to adding the two\u0026rsquo;s complement of the negative number: A - B = A + (-B)\n例如 (Example)：\n1 2 3 4 5 6 5 - 3 = 5 + (-3) 5 的补码 (TC of 5): 0101 -3 的补码 (TC of -3): 1101 ------------------------- 结果 (Result): 0010 (= 2) 溢出检测 (Overflow Detection) 正溢出 (Positive Overflow)\n两个正数相加，结果变成负数 When adding two positive numbers results in a negative number\n负溢出 (Negative Overflow)\n两个负数相加，结果变成正数 When adding two negative numbers results in a positive number\n检测方法 (Detection Methods) 符号位进位法 (Sign Bit Carry Method)\n观察最高位的进位和最高位前一位的进位，如果不同则发生溢出 Compare the carry into and out of the sign bit - if different, overflow occurred\n符号位检查法 (Sign Bit Check Method)\n检查两个操作数符号相同，但结果符号不同 Check if operands have the same sign but result has different sign\n补码的加减法示例 (Examples) 让我们看一个具体的8位补码计算示例： Let\u0026rsquo;s look at an 8-bit two\u0026rsquo;s complement calculation example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1. 计算 5 + (-3)： Calculate 5 + (-3): 5: 00000101 -3: 11111101 ---------------- 结果: 00000010 (= 2) 2. 计算 -5 - 3 = -5 + (-3)： Calculate -5 - 3 = -5 + (-3): -5: 11111011 -3: 11111101 ---------------- 结果: 11110110 (= -8) 定点乘法运算 原码乘法运算 原码乘法运算和手算乘法的程序一样，都是各位相乘以后相加，不多做赘述\n补码乘法运算（Booth算法） 补码乘法运算遵循以下法则：\n对于乘数$x$被乘数$y$，分别求$[x]_补$、$[-x]_补$、$[y]_补$ 初始化$A=0$，$Q=[y]补$，$Q{-1}=0$ 进行如下循环操作： 比较Q0和Q(-1)，根据结果进行操作： 若$Q_0Q_{-1}=00或11$，$AQQ_{-1}$右移 若$Q_0Q_{-1}=10$，$AQQ_{-1}$加$[-x]$后右移 若$Q_0Q_{-1}=01$，$AQQ_{-1}$加$[x]$后右移 操作次数等于乘数的数值位数（不包括符号位） 最后将$A$和$Q$拼接起来 补码乘法小数位置确定 两个n位小数相乘，结果为2n位小数 小数点应放在最高位之前 结果的精度是原始数精度的两倍 定点除法运算 原码除法运算 恢复余数除法 感觉原码直接打竖式算就可以了\n对于被除数$x$和除数$y$，分别求$|x|$、$|x|_补$、$|y|_补$、$[-y]_补$ 用$|x|_补+[-y]_补$，做如下判断 若结果为正，商1，左移 若结果为负，商0，$+[y]_补$，恢复后左移 重复上述操作直到商和机器码长度一致 最后的余数要乘以$2^{左移次数}$ 补码的除法运算 符号位参与运算 被除数、除数、余数采用双符号位 运算步骤 对于被除数$x$和除数$y$，分别求$|x|$、$|y|_补$、$|-y|_补$ 首先判被除数（$x_补$）与除数（$y_补$）是同号还是异号.如果是同号,就要减去（$y_补$）.如果是异号就要加上（$y_补$）. 算出的余数再与除数（$y_补$）进行比较: 如果是同号,商上1,向左移动一位,再减去（$y_补$）,加上（$[-y]_补$） 如果是异号,商上0,向左移动一位,再加上（$y_补$） 循环操作第三步直到商的位数和机器码相同 补码除法的商最后一位固定设置为1 将余数乘以$2^{左移次数}$ 浮点数加减法 运算步骤 对阶 Alignment 将小指数的数向大指数对齐 小指数数的尾数右移，每右移一位，指数加1 直到两个数的指数相等 尾数运算 Mantissa Operation 对阶后进行尾数的加减运算 注意保持正确的符号 规格化 Normalization 调整结果使其满足规格化要求 通常要求尾数最高位为1 可能需要左移或右移尾数 舍入 Rounding 根据舍入规则处理多余的位 溢出检查 Overflow Check 检查结果是否超出表示范围 示例 Examples 例1：基本加法 Basic Addition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 计算：1.101×2² + 1.001×2¹ 1. 对阶： 1.101×2² = 1.101×2² 1.001×2¹ = 0.1001×2² (右移一位) 2. 尾数相加： 1.101 0.1001 ------- 10.0011×2² 3. 规格化： 1.00011×2³ 4. 舍入（假设4位精度）： 1.000×2³ 例2：异号加法（实质是减法）Subtraction 1 2 3 4 5 6 7 8 9 10 11 计算：1.110×2³ - 1.101×2² 1. 对阶： 1.110×2³ = 1.110×2³ 1.101×2² = 0.110×2³ (右移一位) 2. 尾数相减： 1.110 0.110 ------- 1.000×2³ 特殊情况 Special Cases 上溢 Overflow\n1 2 当结果超过最大可表示范围时发生 例如：1.111×2¹²⁷ + 1.111×2¹²⁷ 下溢 Underflow\n1 2 当结果小于最小可表示范围时发生 例如：1.000×2⁻¹²⁶ ÷ 2 精度损失 Precision Loss\n1 2 3 当两个数量级相差很大时： 1.234×2²⁰ + 1.234×2⁻²⁰ 小的数可能完全被忽略 运算方法与运算器 半加法器（Half Adder HA） 全加法器（Full Adder FA） 串行进位全加法器 阵列乘法器 第四章 存储系统 存储器基础概念 存储器分类 按存储介质：半导体、磁性材料、光、纸 按存取方式： 随机存储器(RAM)：存取时间与物理位置无关 顺序存储器：存取时间与物理位置有关(磁盘/光盘/磁带) 按读写方式： RAM (Random Access Memory) ROM (Read Only Memory) 按信息保存性： 永久性(非易失性)：断电不丢失 非永久性(易失性)：断电丢失 主存技术指标 存储容量：存储器所能存储的二进制信息位数 存取速度： 存取时间：启动存取操作到完成的时间 存储周期：连续两次存取操作的最短时间间隔 存储器带宽：单位时间内存取的信息位数 大端小端模式 概念 大端(big-endian)：最高字节地址作为字地址 小端(little-endian)：最低字节地址作为字地址 68000采用大端，Intel采用小端，ARM两者都支持 例题 1 2 3 4 5 6 7 8 9 10 11 12 例：int a = 0x12345678 在内存中的存放方式 大端模式： 0x4000: 0x12 0x4001: 0x34 0x4002: 0x56 0x4003: 0x78 小端模式： 0x4000: 0x78 0x4001: 0x56 0x4002: 0x34 0x4003: 0x12 半导体存储器 SRAM 基本存储单元结构 SRAM 的基本存储单元是由六个晶体管构成的双稳态触发器，通常称为\u0026quot;六管SRAM\u0026quot;或\u0026quot;6T SRAM\u0026quot;。\n主要组成部分 (Main components)：\n4个NMOS管和2个PMOS管 2个交叉耦合的反相器 2个访问晶体管（传输管） 1个位线对（BL和BLB） 1个字线（WL） 工作原理 SRAM的工作原理基于双稳态触发器的特性：\n双稳态特性 (Bistable characteristic)： 两个稳定状态：\u0026lsquo;0\u0026rsquo;和'1\u0026rsquo; 只要有电源供应，数据就能稳定保持 不需要周期性刷新 反相器对 (Inverter pair)： 两个反相器互连形成正反馈 一个节点为高电平时，另一个必为低电平 形成自锁回路 读写过程 写操作 (Write Operation)：\n激活字线(WL) 在位线对(BL/BLB)上施加互补信号 通过访问晶体管强制改变存储节点电平 新数据被锁存 读操作 (Read Operation)：\n预充电位线对至高电平 激活字线 存储单元将数据传递到位线对 感测放大器检测位线电压差 输出数据 SRAM不需要刷新的原因 SRAM不需要刷新的特点源于其结构特性：\n持续性供电 (Continuous Power)： 只要有电源供应，数据就能稳定保持 交叉耦合的反相器持续维持状态 稳定性 (Stability)： 正反馈结构保证数据稳定 不存在电荷泄漏问题 不像DRAM需要定期刷新 DRAM 基本存储单元结构 DRAM 的基本存储单元由一个晶体管和一个电容构成，通常称为\u0026quot;1T1C\u0026quot;结构：\n工作原理 一个晶体管一个电容\nDRAM 的工作原理基于电荷存储：\n数据存储： 1：电容充电（高电平） 0：电容放电（低电平） 电荷泄漏： 电容会随时间缓慢放电 需要定期刷新维持数据 读写过程 写操作 (Write Operation)：\n激活字线(WL)，打开访问晶体管 在位线(BL)上施加数据电平 电容充电或放电 关闭字线，数据被存储 读操作 (Read Operation)：\n位线预充电至中间电平 激活字线，打开访问晶体管 电荷共享导致位线电平变化 感测放大器检测并放大电平差 数据输出并回写（破坏性读出） 刷新操作 刷新周期 (Refresh Cycle) 标准刷新周期：2ms 在2ms内必须完成所有存储单元的刷新 刷新间隔 = 刷新周期 ÷ 行数 1 2 3 4 5 6 7 8 问：一个1024×1024的DRAM芯片，刷新周期为2ms，采用集中刷新方式， 计算刷新时间占用的百分比，假设每次刷新操作耗时100ns。 解： 1. 总行数 = 1024 2. 2ms内需要刷新1024行 3. 总刷新时间 = 1024 × 100ns = 102.4μs 4. 占用百分比 = (102.4μs ÷ 2ms) × 100% = 5.12% 刷新方式 集中刷新 (Burst Refresh)\n工作原理\n在固定的时间段内，连续对所有行进行刷新，期间停止所有正常的存储器访问操作。\n操作流程\n1 2 3 4 5 6 7 8 9 10 11 12 1. 进入刷新周期： - 停止响应所有存储器访问请求 - 将刷新计数器清零 2. 连续刷新过程： - 选择当前行（由刷新计数器指定） - 执行刷新操作 - 刷新计数器+1 - 重复直到所有行刷新完成 3. 退出刷新周期： - 恢复正常的存储器访问 时序图\n例题\n题目： 某DRAM芯片容量为4M×8位，刷新周期要求为4ms，每次刷新操作需要100ns。该DRAM采用集中刷新方式，要求：\n计算需要多少根地址线 计算总的刷新时间 计算在刷新期间的存储器利用率损失 如果CPU时钟周期为10ns，计算在一个刷新周期内损失的CPU周期数 解答：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1. 地址线计算： - 总容量 = 4M×8位 = 4194304×8位 - 行地址需要：log₂(√4194304) = 11位 - 列地址需要：log₂(√4194304) = 11位 - 总地址线：11 + 11 = 22根 2. 刷新时间计算： - 行数 = 2¹¹ = 2048行 - 总刷新时间 = 2048行 × 100ns = 204.8μs 3. 存储器利用率损失： - 刷新周期 = 4ms = 4000μs - 利用率损失 = (204.8μs ÷ 4000μs) × 100% = 5.12% 4. 损失的CPU周期： - 刷新时间 = 204.8μs = 204800ns - 损失的CPU周期数 = 204800ns ÷ 10ns = 20480个周期 分散刷新 (Distributed Refresh)\n工作原理\n在每个存储器正常访问周期结束后，插入一次刷新操作。\n操作流程\n1 2 3 4 5 6 7 8 1. 执行正常的存储器访问操作 2. 访问周期结束后： - 执行一次刷新操作（刷新一行） - 刷新计数器+1 - 为下一行刷新做准备 3. 开始下一个访问周期 时序图\n例题\n题目： 某DRAM芯片为16M×4位，刷新周期为2ms，采用分散刷新方式。存储器的读写周期为80ns，刷新操作需要100ns。请计算：\n一个完整的存储器周期时间 实际的存储器带宽 如果改用集中刷新，两种方式的存储器利用率比较 刷新操作的频率 解答：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 1. 存储器周期计算： - 行数 = √(16M) = 4096行 - 每个刷新周期内需要刷新次数 = 4096次 - 每次刷新间隔 = 2ms ÷ 4096 ≈ 488ns - 完整周期 = 80ns(读写) + 100ns(刷新) = 180ns 2. 实际带宽计算： - 理论周期时间 = 80ns - 实际周期时间 = 180ns - 理论带宽 = (4位 ÷ 80ns) = 50MB/s - 实际带宽 = (4位 ÷ 180ns) ≈ 22.22MB/s 3. 刷新方式比较： 分散刷新： - 利用率 = 80ns ÷ 180ns ≈ 44.4% 集中刷新： - 总刷新时间 = 4096 × 100ns = 409.6μs - 利用率 = (2000μs - 409.6μs) ÷ 2000μs ≈ 79.52% 4. 刷新频率： - 每488ns进行一次刷新 - 刷新频率 = 1 ÷ 488ns ≈ 2.049MHz 异步刷新 (Asynchronous Refresh)\n工作原理\n将刷新周期分成多个时间片，每个时间片内完成部分行的刷新。\n操作流程\n1 2 3 4 5 6 7 8 9 10 11 1. 刷新周期分段： - 通常分为8个或16个时间片 - 每个时间片负责特定数量的行刷新 2. 每个时间片内： - 连续刷新分配给该时间片的所有行 - 其他时间用于正常访问 3. 时间片切换： - 完成当前时间片的刷新任务 - 等待下一个时间片 时序图\n异步刷新例题\n题目： 某DRAM容量为64M×8位，刷新周期4ms，采用异步刷新方式，将刷新周期分为8个时间片。每次刷新操作需要120ns，普通读写操作需要100ns。请计算：\n每个时间片的持续时间和需要刷新的行数 每个时间片内的存储器利用率 如果要求系统响应时间不超过1μs，判断是否满足要求 计算平均访问时间 解答：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 1. 时间片计算： - 总行数 = √64M = 8192行 - 每个时间片时长 = 4ms ÷ 8 = 500μs - 每个时间片需刷新行数 = 8192 ÷ 8 = 1024行 - 每个时间片的刷新时间 = 1024 × 120ns = 122.88μs 2. 存储器利用率： - 每个时间片可用时间 = 500μs - 122.88μs = 377.12μs - 单片利用率 = 377.12μs ÷ 500μs ≈ 75.424% 3. 响应时间分析： - 最坏情况：刷新操作正在进行时收到访问请求 - 最大等待时间 = 1024行 × 120ns = 122.88μs - 122.88μs \u0026gt; 1μs，不满足要求 4. 平均访问时间： - 正常访问时间 = 100ns - 刷新影响时间 = (122.88μs ÷ 500μs) × 100ns = 24.576ns - 平均访问时间 = 100ns + 24.576ns = 124.576ns 三种方式的比较 特性 集中刷新 分散刷新 异步刷新 控制复杂度 简单 复杂 中等 存储器利用率 低 高 中等 实现难度 容易 困难 中等 刷新效率 高 低 中等 对系统影响 大 小 中等 主存与CPU的连接 存储器扩展 位扩展 (Bit Extension) 当存储器的数据位宽不足以满足系统需求时，需要进行位扩展。\n原理 (Principle)\n将多个存储器芯片并联，增加数据位宽 所有芯片共用相同的地址线和控制线 每个芯片负责存储数据的不同位 示例 (Example)\n将两个8位宽的存储器芯片组成16位宽的存储器：\n芯片1：存储D0-D7 芯片2：存储D8-D15 共用地址线A0-An 共用片选信号CS、读写控制信号R/W 字扩展 (Word Extension) 当存储器容量（字数）不足时，需要进行字扩展。\n原理 (Principle)\n将多个存储器芯片串联，增加存储容量 所有芯片共用数据线和控制线 通过片选信号选择不同的芯片 示例 (Example)\n将两个1K×8位的存储器组成2K×8位的存储器：\n使用A10作为片选信号 A10=0选择芯片1（地址0-1023） A10=1选择芯片2（地址1024-2047） 字位同时扩展 (Word-Bit Simultaneous Extension) 当需要同时扩展数据位宽和存储容量时使用。\n原理 (Principle)\n结合位扩展和字扩展的方法 形成一个矩阵式的存储器阵列 需要同时考虑数据位的分配和片选逻辑 示例 (Example)\n用4个1K×8位的存储器组成2K×16位的存储器：\n水平方向：位扩展（8位→16位） 垂直方向：字扩展（1K→2K） 片选方式 (Chip Selection Methods) 线选法 (Linear Selection)\n线选法是最简单的片选方式，直接用一根选通线来选中一个存储芯片。每个存储芯片都有独立的片选信号线。\n优点：\n电路简单，容易实现 选择速度快 缺点：\n需要的片选线数量多 当存储芯片数量增加时，占用的I/O端口也相应增加 扩展能力受限 全译码法 (Full Decoding)\n全译码法使用地址译码器将n位二进制地址译码成$2^n$个片选信号。\n优点：\n片选线使用效率高 每个地址都对应唯一的存储芯片 地址空间利用率100% 特点：\nn根地址线可以选择$2^n$个存储芯片 需要使用译码器（如74138） 地址连续，没有空隙 部分译码法 (Partial Decoding)\n部分译码法只使用地址的部分位进行译码，一个片选信号可能对应多个地址。\n优点：\n电路比全译码简单 需要的译码器更少 适合于地址空间不需要完全使用的场合 缺点：\n地址空间有重复 存储器的编址不连续 地址空间利用率低 例题\n例1（字扩展）：用16K8位的芯片，去构造内存64K8，并要完成与CPU的对接，并求出每一个芯片在全局空间中的地址范围。\n解答：\n需要的芯片数量：\n1 64K ÷ 16K = 4片芯片 地址范围：\n总地址范围：0000H ~ FFFFH (64K = 2^16) 每片芯片负责16K地址空间 16K = 16384 = 4000H 地址分配表 / Address Allocation Table\n芯片号/No. 地址范围 / Address Range 16进制值/Hex Value A15 A14 A13~A0 0 0 0 0000~3FFF 00000H ~ 03FFFH 1 0 1 0000~3FFF 04000H ~ 07FFFH 2 1 0 0000~3FFF 08000H ~ 0BFFFH 3 1 1 0000~3FFF 0C000H ~ 0FFFFH 解释说明 / Explanation\n地址线分配 / Address Line Assignment: A15, A14：用于片选（选择哪个芯片） A13~A0：用于芯片内部寻址（14位） 片选逻辑 / Chip Selection Logic: 芯片0：当A15=0, A14=0时选中 芯片1：当A15=0, A14=1时选中 芯片2：当A15=1, A14=0时选中 芯片3：当A15=1, A14=1时选中 地址范围计算 / Address Range Calculation: 每片芯片负责16K (4000H) 的地址空间 芯片0：0000H ~ 3FFFH 芯片1：4000H ~ 7FFFH 芯片2：8000H ~ BFFFH 芯片3：C000H ~ FFFFH 例2（位扩展）：用256K ×8位的存储体构造 2M × 32位的存储器，并完成与CPU的连接\n先用4片256K X 8位的存储体构成 256K X 32的存储体\n然后和字扩展一样\nCache 工作原理 / How Cache Works 数据存储： Cache将最常用的数据从主存复制到高速缓存中 按照块（Block）或行（Line）为单位进行存储 访问过程： CPU首先查找数据是否在Cache中（Cache命中） 如果命中（Hit），直接从Cache读取 如果未命中（Miss），从主存读取并放入Cache 替换策略： 当Cache满时，需要决定替换哪些数据 常见策略包括LRU（最近最少使用）、FIFO（先进先出）等 局局部性原理 / Principle of Locality 时间局部性 / Temporal Locality\n定义：如果一个数据被访问，那么在近期它很可能再次被访问 例子： 循环中的变量 频繁调用的函数 计数器 1 2 3 for(int i = 0; i \u0026lt; 100; i++) { // i具有很好的时间局部性 sum += array[i]; } 空间局部性 / Spatial Locality\n定义：如果一个数据被访问，那么它周围的数据很可能也会被访问 例子： 数组的连续访问 顺序执行的指令 结构体中的相邻成员 1 2 3 4 int array[100]; for(int i = 0; i \u0026lt; 100; i++) { array[i] = i; // 数组连续访问体现了空间局部性 } 局部性原理的重要性 / Importance of Locality 性能优化： 利用局部性原理可以提高Cache命中率 减少主存访问次数，提升系统性能 程序设计： 影响程序编写方式 引导更好的数据结构和算法选择 实际应用示例 / Practical Example 1 2 3 4 5 6 7 8 9 10 11 12 13 // 体现良好局部性的代码 for(int i = 0; i \u0026lt; N; i++) { for(int j = 0; j \u0026lt; N; j++) { array[i][j] = 0; // 按行访问，具有好的空间局部性 } } // 较差局部性的代码 for(int j = 0; j \u0026lt; N; j++) { for(int i = 0; i \u0026lt; N; i++) { array[i][j] = 0; // 按列访问，空间局部性较差 } } 这个例子展示了如何通过合适的访问模式来利用空间局部性，提高程序性能。第一种方式（按行访问）能更好地利用Cache的特性，而第二种方式（按列访问）会导致更多的Cache未命中。\n存储器映射方式 Memory Mapping Methods 直接映射 (Direct Mapping)\n基本概念\n直接映射是最简单的映射方式，主存中的每个块只能映射到Cache中的一个特定位置。\n地址结构\n主存地址分为三个字段：\n标记(Tag)：用于识别是否是所需的块 组号/行号(Line)：确定Cache中的位置 块内地址(Block Offset)：确定块内的具体单元 例题 Example\n假设有一个存储系统具有以下参数：\n主存容量：1024KB = 2^20 B Cache容量：16KB = 2^14 B 块大小：64B = 2^6 B 解答：\n计算地址位数： 主存地址位数 = log2(1024×1024) = 20位 计算各字段位数： 块内地址：log2(64) = 6位 Cache行数：16KB/64B = 256行，所以行号需要8位 标记位：20 - 8 - 6 = 6位 地址格式： 1 | 标记(6位) | 行号(8位) | 块内地址(6位) | 全相联映射 (Fully Associative Mapping)\n基本概念 Basic Concept\n主存中的任何一块可以映射到Cache中的任何位置。需要并行对比所有Cache行。\n地址结构 Address Structure\n主存地址分为两个字段：\n标记(Tag) 块内地址(Block Offset) 例题 Example\n使用上述相同参数，求地址格式。\n解答\n1. 块内地址：同样是6位 2. 标记位： - 不需要行号字段 - 标记位 = 20 - 6 = 14位 3. 地址格式： ``` | 标记(14位) | 块内地址(6位) | ``` 组相联映射 (Set Associative Mapping)\n基本概念 Basic Concept\n是直接映射和全相联映射的折中方案。Cache分成若干组，每组包含n个行（n路组相联）。\n地址结构 Address Structure\n主存地址分为三个字段：\n标记(Tag) 组号(Set) 块内地址(Block Offset) 例题 Example\n假设采用4路组相联，其他参数同上。\n解答：\n1. 计算组数： - 总行数 = 256行 - 每组4行 - 组数 = 256/4 = 64组 2. 计算各字段位数： - 块内地址：6位 - 组号：log2(64) = 6位 - 标记位：20 - 6 - 6 = 8位 3. 地址格式： ``` | 标记(8位) | 组号(6位) | 块内地址(6位) | ``` 性能比较 Performance Comparison 命中率 Hit Rate： 全相联 \u0026gt; 组相联 \u0026gt; 直接映射 硬件复杂度 Hardware Complexity： 直接映射 \u0026lt; 组相联 \u0026lt; 全相联 查找速度 Search Speed： 直接映射 \u0026gt; 组相联 \u0026gt; 全相联 综合例题 Comprehensive Example 假设主存地址为32位，Cache大小为64KB，块大小为32B，采用8路组相联映射，求：\n地址格式 Cache总行数 每组行数 组数 标记位、组号位、块内地址位的位数 解答 Solution:\nCache总行数： 64KB/32B = 2048行 每组行数： 8行（8路组相联） 组数： 2048/8 = 256组 地址位的分配： 块内地址：log2(32) = 5位 组号：log2(256) = 8位 标记：32 - 8 - 5 = 19位 地址格式： 1 | 标记(19位) | 组号(8位) | 块内地址(5位) | 替换算法 LRU 工作原理\n替换最长时间没有被访问的数据块 需要记录每个数据块的最后访问时间 基于程序的时间局部性原理 实现方式\n计数器法：记录上次访问时间 栈实现：最近使用的放栈顶 链表实现：访问后移至表头 例题\n假设有一个容量为 3 的 Cache，访问序列为：1, 2, 3, 4, 1, 2, 5, 1, 2, 3\n1 2 3 4 5 6 7 8 初始状态：[ ][ ][ ] 1 访问 → [1][ ][ ] 2 访问 → [2][1][ ] 3 访问 → [3][2][1] 4 访问 → [4][3][2] (替换最久未使用的1) 1 访问 → [1][4][3] (替换最久未使用的2) 2 访问 → [2][1][4] (替换最久未使用的3) 5 访问 → [5][2][1] (替换最久未使用的4) LFU 工作原理\n替换访问次数最少的数据块 需要维护访问计数器 基于使用频率进行判断 例题\n假设有一个容量为 3 的 Cache，访问序列为：1, 1, 1, 2, 2, 3, 4, 1\n1 2 3 4 5 6 7 8 初始状态：[ ][ ][ ] 1 访问 → [1:1][ ][ ] (数字:计数) 1 访问 → [1:2][ ][ ] 1 访问 → [1:3][ ][ ] 2 访问 → [1:3][2:1][ ] 2 访问 → [1:3][2:2][ ] 3 访问 → [1:3][2:2][3:1] 4 访问 → [1:3][2:2][4:1] (替换计数最小的3) FIFO 工作原理\n替换最早进入 Cache 的数据块 类似队列操作 实现简单，硬件开销小 例题\n假设有一个容量为 3 的 Cache，访问序列为：1, 2, 3, 4, 2, 1, 5\n1 2 3 4 5 6 7 8 初始状态：[ ][ ][ ] 1 进入 → [1][ ][ ] 2 进入 → [1][2][ ] 3 进入 → [1][2][3] 4 进入 → [4][2][3] (替换最早进入的1) 2 访问 → [4][2][3] (已存在，不变) 1 进入 → [4][2][1] (替换最早进入的3) 5 进入 → [4][5][1] (替换最早进入的2) 随机替换 工作原理\n随机选择要替换的数据块 不需要维护任何历史信息 硬件实现最简单 例题\n假设有一个容量为 3 的 Cache，访问序列为：1, 2, 3, 4, 2, 1, 5\n过程演示 (Process Demonstration) 1 2 3 4 5 6 7 8 初始状态：[ ][ ][ ] 1 进入 → [1][ ][ ] 2 进入 → [1][2][ ] 3 进入 → [1][2][3] 4 进入 → [4][2][3] (随机替换了1) 2 访问 → [4][2][3] (已存在，不变) 1 进入 → [4][1][3] (随机替换了2) 5 进入 → [4][1][5] (随机替换了3) 性能比较 写策略 写直达法(Write Through) 数据同时写入 Cache 和主存 每次写操作都要访问主存 Cache 和主存的数据始终保持一致 写回法(Write Back) 写操作只修改 Cache 中的数据 被修改的数据块被标记为\u0026quot;脏\u0026quot;(dirty) 当脏块被替换时，才写回主存 性能计算 命中率计算 在一个程序执行期间，设$N_c$表示Cache完成存取的总次数，$N_m$表示主存完成存取的总次数，h定义为命中率，则有\n$$ h=\\frac{N_c}{N_c+N_m} $$平均访问时间计算 若$t_c$表示命中时的Cache访问时间，$t_m$表示未命中时的主存访问时间，$1-h$表示未命中率，则Cache/主存系统的平均访问时间$t_a$为\n$$ t_a=h\\cdot t_c+(1-h)t_m　$$Cache效率计算 设$r=\\frac{tm}{tc}$表示主存慢于Cache的倍率,e表示访问效率，则有:\n$$ e = \\frac{t_c}{t_a} = \\frac{t_c}{ht_c + (1-h)t_m} = \\frac{1}{r + (1-r)h} = \\frac{1}{h + (1-h)r} $$","date":"0001-01-01T00:00:00Z","image":"https://whale-dolphin.github.io/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%E5%A4%8D%E4%B9%A0/cover_hu5607236142407653422.png","permalink":"https://whale-dolphin.github.io/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%E5%A4%8D%E4%B9%A0/","title":"计算机组成原理复习"}]